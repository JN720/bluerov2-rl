{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rclpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normal\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrclpy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rclpy'"
     ]
    }
   ],
   "source": [
    "!source /opt/ros/humble/setup.bash\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "import rclpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotActor(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(RobotActor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(3, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class RobotCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobotCritic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(3, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(x, y, distance, goal_distance):\n",
    "    reward = np.abs(goal_distance - distance) + np.sqrt(2) - np.sqrt((x ** 2) + (y ** 2)) + 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thrust import EnvManager, ThrusterCommandPublisher, PositionReader\n",
    "import time\n",
    "\n",
    "class GzEnv():\n",
    "    def __init__(self):\n",
    "        self.env_manager = EnvManager()\n",
    "        self.position_reader = PositionReader(self.env_manager)\n",
    "        self.thruster_command_publisher = ThrusterCommandPublisher(self.position_reader)\n",
    "        self.goal_distance = 3\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute the action and wait a bit\n",
    "        self.thruster_command_publisher.publish(action)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        reward = reward_function()\n",
    "        x, y, distance = self.position_reader.get_observation()\n",
    "        self.observation = np.array([x, y, distance], dtype = np.float32)\n",
    "        \n",
    "        return self.observation, reward, np.random.rand() > 0.95, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.env_manager.reset()\n",
    "        x, y, distance = self.position_reader.get_observation()\n",
    "        self.observation = np.array([x, y, distance], dtype = np.float32)\n",
    "\n",
    "        return self.observation - np.array([0, 0, -self.goal_distance], dtype = np.float32), {}\n",
    "    \n",
    "    def close(self):\n",
    "        self.env_manager.destroy_node()\n",
    "        self.position_reader.destroy_node()\n",
    "        self.thruster_command_publisher.destroy_node()\n",
    "        rclpy.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, input_dims, alpha, gamma, epsilon, gae_lambda, epochs, batch_size, learn_iters):\n",
    "        self.actor = RobotActor(actions)\n",
    "        self.critic = RobotCritic()\n",
    "        self.action_size = actions\n",
    "        self.input_dims = input_dims\n",
    "        self.actor.opt = torch.optim.Adam(self.actor.parameters(), lr = alpha)\n",
    "        self.critic.opt = torch.optim.Adam(self.critic.parameters(), lr = alpha)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_iters = learn_iters\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def reset_mem(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def store_mem(self, state, prob, action, reward, value, done):\n",
    "        self.states.append(state.tolist())\n",
    "        self.probs.append(prob)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def batching(self):\n",
    "        batch_size_indices = np.arange(0, len(self.states), self.batch_size)\n",
    "        batch_indices = np.arange(0, self.batch_size, dtype = np.int64)\n",
    "        np.random.shuffle(batch_indices)\n",
    "        batch_indices = batch_indices.tolist()\n",
    "        rvalue = []\n",
    "        for i in batch_size_indices:\n",
    "            rvalue += batch_indices[i:i + self.batch_size]\n",
    "        return np.array(rvalue, dtype = np.int64)\n",
    "        \n",
    "    def actt(self, obs):\n",
    "        obs = obs.unsqueeze(0)\n",
    "        policy = self.actor(obs).squeeze()\n",
    "        policy1 = Categorical(F.softmax(policy[0:5], dim = -1))\n",
    "        policy2 = Categorical(F.softmax(policy[5:8], dim = -1))\n",
    "        policy3 = Categorical(F.softmax(policy[8:10], dim = -1))\n",
    "        policy4 = Categorical(F.softmax(policy[10:12], dim = -1))\n",
    "        value = self.critic(obs).squeeze().item()\n",
    "        #action and log probs will be of size 3\n",
    "        action1 = policy1.sample()\n",
    "        action2 = policy2.sample()\n",
    "        action3 = policy3.sample()\n",
    "        action4 = policy4.sample()\n",
    "        #since these log probs are passed directly into store mem,\n",
    "        #and the same is done with the new probs, only the sum is returned\n",
    "        prob1 = policy1.log_prob(action1).item()\n",
    "        prob2 = policy2.log_prob(action2).item()\n",
    "        prob3 = policy3.log_prob(action3).item()\n",
    "        prob4 = policy3.log_prob(action3).item()\n",
    "        \n",
    "        return [action1.item(), action2.item(), action3.item(), action4.item()], prob1 + prob2 + prob3 + prob4, value\n",
    "        \n",
    "    def learnn(self):\n",
    "        advantage = np.zeros(len(self.rewards) - 1, dtype = np.float32)\n",
    "        self.states = np.array(self.states, dtype = np.float32)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            batch_indices = np.array(self.batching(), dtype = np.int64)\n",
    "            #gae\n",
    "            #summation of memory\n",
    "            for j in range(len(self.rewards) - 1):\n",
    "                #delta coefficient\n",
    "                discount = 1\n",
    "                #advantage\n",
    "                a = 0\n",
    "                for k in range(j, len(self.rewards) - 1):\n",
    "                    #delta of timestep = (done coefficient * gamma * next state value) + reward - current state value\n",
    "                    #basically new value + reward - cur value\n",
    "                    a += discount * (((1 - self.dones[k]) * self.gamma * self.values[k + 1]) + self.rewards[k] - self.values[k])\n",
    "                    #gae lamba^n * gamma^n\n",
    "                    discount *= self.gamma * self.gae_lambda\n",
    "                #advantage at each timestep\n",
    "                advantage[j] = a\n",
    "            advantage = torch.Tensor(advantage).float()\n",
    "            state_batches = []\n",
    "            p1 = []\n",
    "            ab = []\n",
    "            vb = []\n",
    "            advantage_batch = []\n",
    "            #sampling of random memory\n",
    "            for i in batch_indices:\n",
    "                state_batches.append(self.states[i])\n",
    "                p1.append(self.probs[i])\n",
    "                ab.append(self.actions[i])\n",
    "                vb.append(self.values[i])\n",
    "                advantage_batch.append(advantage[i])\n",
    "            state_batches = torch.Tensor(np.array(state_batches)).float()\n",
    "            #these 2 are size 4 for the multi discrete implementation\n",
    "            p1 = torch.Tensor(np.array(p1)).float()\n",
    "            ab = torch.Tensor(np.array(ab)).long()\n",
    "            vb = torch.Tensor(np.array(vb)).float()\n",
    "            advantage_batch = torch.Tensor(advantage_batch).float()\n",
    "            #predictions\n",
    "            apred = self.actor(state_batches)\n",
    "            apred1 = Categorical(F.softmax(apred[0, 0:5], dim = -1))\n",
    "            apred2 = Categorical(F.softmax(apred[0, 5:8], dim = -1))\n",
    "            apred3 = Categorical(F.softmax(apred[0, 8:10], dim = -1))\n",
    "            apred4 = Categorical(F.softmax(apred[0, 10:12], dim = -1))\n",
    "            cpred = self.critic(state_batches)\n",
    "            #get new log probs corresponding to past actions from memory\n",
    "            #there are 3 of these now\n",
    "            #in the 37 implementation details thingy, they multiplied the probs for each distribution\n",
    "            #since these are logits, they shall be added\n",
    "            p2 = apred1.log_prob(ab[:, 0]) + apred2.log_prob(ab[:, 1]) + apred3.log_prob(ab[:, 2]) + apred4.log_prob(ab[:, 3])\n",
    "            #actor loss calculation: this is the same now that the probs are combined\n",
    "            pratio = p2.exp() / p1.exp()\n",
    "            wpratio = pratio * advantage_batch\n",
    "            cwpratio = torch.clamp(pratio, 1 - self.epsilon, 1 + self.epsilon) * advantage_batch\n",
    "            aloss = (-torch.min(wpratio, cwpratio)).mean()\n",
    "            #critic loss: gae + state value MSE'd with raw network prediction\n",
    "            #gae + state value = new state + reward\n",
    "            #in other words, optimize state value to become new state + reward\n",
    "            ctarget = advantage_batch + vb\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            #closs = ((ctarget - cpred) ** 2).mean()\n",
    "            closs = criterion(ctarget.unsqueeze(-1), cpred)\n",
    "            #now includes entropy term\n",
    "            entropy = (0.1 * apred1.entropy()) + (0.8 * apred2.entropy()) + (0.2 * apred3.entropy()) + (0.1 * apred4.entropy())\n",
    "            loss = aloss + (0.5 * closs) - (0.4 * entropy)\n",
    "            self.actor.opt.zero_grad()\n",
    "            self.critic.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor.opt.step()\n",
    "            self.critic.opt.step()\n",
    "        self.reset_mem()\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = obs.unsqueeze(0)\n",
    "        # Actor outputs mean and log standard deviation for each of the 6 thrusters\n",
    "        mean, log_std = self.actor(obs)\n",
    "        std = log_std.exp()  # Convert log std to std\n",
    "        # Create a Gaussian distribution for each thruster\n",
    "        dist = Normal(mean, std)\n",
    "        # Sample actions for all 6 thrusters\n",
    "        action = dist.sample()\n",
    "        # Compute log probability of the sampled actions\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1).item()  # Sum log probs across thrusters\n",
    "        # Get the value estimate from the critic\n",
    "        value = self.critic(obs).squeeze().item()\n",
    "        return action.tolist(), log_prob, value\n",
    "\n",
    "    def learn(self):\n",
    "        advantage = np.zeros(len(self.rewards) - 1, dtype=np.float32)\n",
    "        self.states = np.array(self.states, dtype=np.float32)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            batch_indices = np.array(self.batching(), dtype=np.int64)\n",
    "            # Compute GAE (Generalized Advantage Estimation)\n",
    "            for j in range(len(self.rewards) - 1):\n",
    "                discount = 1\n",
    "                a = 0\n",
    "                for k in range(j, len(self.rewards) - 1):\n",
    "                    a += discount * (((1 - self.dones[k]) * self.gamma * self.values[k + 1]) + self.rewards[k] - self.values[k])\n",
    "                    discount *= self.gamma * self.gae_lambda\n",
    "                advantage[j] = a\n",
    "            advantage = torch.Tensor(advantage).float()\n",
    "\n",
    "            # Prepare batches\n",
    "            state_batches = []\n",
    "            old_log_probs = []\n",
    "            action_batches = []\n",
    "            value_batches = []\n",
    "            advantage_batches = []\n",
    "            for i in batch_indices:\n",
    "                state_batches.append(self.states[i])\n",
    "                old_log_probs.append(self.probs[i])\n",
    "                action_batches.append(self.actions[i])\n",
    "                value_batches.append(self.values[i])\n",
    "                advantage_batches.append(advantage[i])\n",
    "\n",
    "            state_batches = torch.Tensor(np.array(state_batches)).float()\n",
    "            old_log_probs = torch.Tensor(np.array(old_log_probs)).float()\n",
    "            action_batches = torch.Tensor(np.array(action_batches)).float()\n",
    "            value_batches = torch.Tensor(np.array(value_batches)).float()\n",
    "            advantage_batches = torch.Tensor(advantage_batches).float()\n",
    "\n",
    "            # Predict new actions and values\n",
    "            mean, log_std = self.actor(state_batches)\n",
    "            std = log_std.exp()\n",
    "            dist = Normal(mean, std)\n",
    "            new_log_probs = dist.log_prob(action_batches).sum(dim=-1)\n",
    "            entropy = dist.entropy().sum(dim=-1).mean()  # Sum entropy across thrusters and average over batch\n",
    "\n",
    "            # Actor loss (clipped surrogate objective)\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage_batches\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage_batches\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Critic loss (MSE between predicted and target values)\n",
    "            value_pred = self.critic(state_batches).squeeze()\n",
    "            target_values = advantage_batches + value_batches\n",
    "            critic_loss = F.mse_loss(value_pred, target_values)\n",
    "\n",
    "            # Total loss\n",
    "            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy  # Adjust entropy coefficient as needed\n",
    "\n",
    "            # Update networks\n",
    "            self.actor.opt.zero_grad()\n",
    "            self.critic.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor.opt.step()\n",
    "            self.critic.opt.step()\n",
    "\n",
    "        self.reset_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 6\n",
    "INPUT_DIMS = 4\n",
    "LR = 5e-4\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "POLICY_CLIP = 0.1\n",
    "SMOOTHING = 0.95\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 5\n",
    "LEARN_ITERS = 20\n",
    "\n",
    "env = GzEnv()\n",
    "\n",
    "agent = Agent(ACTIONS, INPUT_DIMS, LR, DISCOUNT_FACTOR, POLICY_CLIP, SMOOTHING, EPOCHS, BATCH_SIZE, LEARN_ITERS)\n",
    "\n",
    "EPISODES = 5\n",
    "\n",
    "steps = 0\n",
    "nobs = 0\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    obs = torch.tensor(env.reset()[0])\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, value = agent.act(obs)\n",
    "        nobs, reward, done, _, _ = env.step(action)\n",
    "        nobs = torch.tensor(nobs)\n",
    "        agent.store_mem(obs, prob, action, reward, value, done)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "        obs = nobs\n",
    "        if steps % agent.learn_iters == 0:\n",
    "            agent.learn()\n",
    "    print(\"Episode: {} Scores: {}, {}\".format(i + 1, score))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
